{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain can be useful for template-based language generation tasks, and it offers flexibility in integrating different language models, including Google's Gemini and OpenAI's models. Here's how you can use langchain to easily switch between Gemini and OpenAI models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Define context and questions\n",
    "context = \"Albert Einstein was a theoretical physicist who developed the theory of relativity.\"\n",
    "questions = [\n",
    "    \"What did Albert Einstein develop?\",\n",
    "    \"Who was Albert Einstein?\",\n",
    "    \"When was Albert Einstein born?\"\n",
    "]\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are providing information about Albert Einstein. Answer the question as accurately as possible using the provided context. If the answer is not contained in the context, say \"answer not available in context\".\n",
    "\n",
    "Context: \\n{context}\\n\n",
    "Question: \\n{question}\\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Function to answer questions using Gemini\n",
    "def answer_question_with_gemini(question, context):\n",
    "    # Initialize the model\n",
    "    model = \"gemini-pro\"  # Specify the Gemini model\n",
    "\n",
    "    # Initialize the language model chain\n",
    "    qa_chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "    response = qa_chain({\"context\": context, \"question\": question})\n",
    "    return response['text']\n",
    "\n",
    "# Function to answer questions using OpenAI\n",
    "def answer_question_with_openai(question, context):\n",
    "    # Initialize the model\n",
    "    model = \"text-davinci-003\"  # Specify the OpenAI model\n",
    "\n",
    "    # Initialize the language model chain\n",
    "    qa_chain = LLMChain(llm=model, prompt=prompt)\n",
    "\n",
    "    response = qa_chain({\"context\": context, \"question\": question})\n",
    "    return response['text']\n",
    "\n",
    "# Answer the questions using Gemini\n",
    "print(\"Answering questions using Gemini:\")\n",
    "for question in questions:\n",
    "    answer = answer_question_with_gemini(question, context)\n",
    "    print(f\"Q: {question}\\nA: {answer}\\n\")\n",
    "\n",
    "# Answer the questions using OpenAI\n",
    "print(\"Answering questions using OpenAI:\")\n",
    "for question in questions:\n",
    "    answer = answer_question_with_openai(question, context)\n",
    "    print(f\"Q: {question}\\nA: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I've defined separate functions for answering questions using Gemini and OpenAI models. By specifying the desired model in each function, you can easily switch between Gemini and OpenAI for answering questions while maintaining the same prompt template and overall structure"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
